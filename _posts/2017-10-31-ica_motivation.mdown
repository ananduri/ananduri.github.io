# Independent Component Analysis

Independent Component Analysis (ICA) is an unsupervised learning method, and if you haven't heard of it, you've probably heard of its more famous cousin Principle Component Analysis (PCA). PCA has a high ratio of usefulness to mathematical difficulty, and it's often introduced as a paradigmatic example of unsupervised learning. ICA is not as straightforward, but that also means it seems even more like magic. Consider this picture from Hastie, Tibshirani, Friedman [1]:

![ICA vs PCA](ICA_PCA.png)

Clearly PCA is hopeless at separating the two signals, whereas ICA recovers them perfectly. Given that PCA is usually presented alongside examples that clearly illustrate its utility, and make PCA itself seem magical, this example only made ICA seem even more mysterious and magical. Furthermore, I was wondering if it was related to PCA--their names couldn't just be a coincidence, right? So I decided to dive in, and write up my thoughts here. A friend recently told me "Muddled thoughts lead to muddled writing." Since the contrapositive is also true, writing this post will only help me understand ICA better.


## Motivation

Q: Why would one need to know how to do ICA? 
A: Well, it's a basic form of unsupervised learning. 
Q: Why would one need to know how to do unsupervised learning?

I'm going to motivate ICA by first providing my answer to the second question above. 

### Unsupervised learning

What is unsupervised learning? One way to think about it is a transformation process. You are given some data, and you want to transform it to another representation, where you can "interpret" it better. What does "interpret" mean? It means that you assign meaning to the data. In other words, you create a higher-level abstraction, and use your label for that abstraction to stand for some of the data. You're using something simpler, like a word, to represent something unwieldy, like data in the form of lots and lots of numbers. 

You do this so that in the future, if you're reasoning about something else and this abstraction plays a role, you can perform a calculation involving the abstraction by plugging in the labeled data, which you used unsupervised learning to isolate.

Here's a concrete example of this high-level discussion: You're in kindergarten. The teacher is saying something about an "alphabet", and presents you with a visual image containing 26 drawings. This is your data. You learn to tell each of those drawings apart, and can make the right sound when you see one by itself in the future--this is you interpreting the data. In your mind, you've transformed the visual image of the page with the entire alphabet on it, to a collection of 26 different individual images. Then, you teacher created the higher-level abstractions of the letters A-Z, and helped you assign a label for that abstraction--the name of the letter--to each of the 26 different shapes. You're using something simple, like the sound "ay", so that you don't have to keep saying "the up-pointing shape with a line through the middle".

Now in the future, when you're writing down your name, you first think, "I need to write the first letter of my name, which is the letter A." This is the separate calculation involving the abstraction you created earlier. Now you can plug in the data you labeled, which is the visual information of "the up-pointing shape with a line through the middle". By drawing that shape, labeled in your mind with the sound "ay", you can finish the calculation and write your name.

### ICA

That was a really high-level discussion, and while those are fun to think about, let's try to get more concrete. Above, I said unsupervised learning is a transformation process. Given some data, you want to _transform_ it to another _representation_. There are two avenues to making this statement more concrete: we can answer "transform it how?" and "what criteria do you want this other representation to have?"

The simplest thing to consider at first might be linear transforms. This means that given some data, we're only going to transform it by multiplying the numbers in that data by some constants, and/or adding the results of those multiplications together. Another way to understand this is by representing each data point as a vector, where the elements of the vector are the values of each attribute of the data point (I'm assuming multi-dimensional data for this post). By restricting yourself to linear transforms, you're saying that you're only going to multiply this vector by a matrix, and use the same matrix for all your data points. (The matrix itself can change depending on the entire data set, though).

Next, we have to specify the characteristics we want the transformed representation of the data to have. This is the interesting part. Behind many transformations are assumptions: you act as though a certain scenario is true, which allows you to actually do something to your data, and then when you're finished transforming the data, you check to see if the output is actually consistent with the assumptions you made in the beginning. This is also the case for ICA.

We're going to assume that the data is generated by a number of 'sources'. Each source outputs a signal that various measurement devices of ours pick up. You can think of the reading of each measurement device as one element of our data. For example, each 'source' could be the voice of certain people in a room, and our measurement devices could be a series of microphones [2]. Each microphone is placed in a different location in the room. Since every person is a different distance from all of the microphones, each source is picked up in a unique way by each of the microphones. Mathematically, a signal from a given source is attenuated by a constant factor when it is picked up by a given microphone. For a given source, the set of these factors tell you how our measurement devices pick up its signal. If you consider the vector space where our data lives, this set of factors picks out a unique direction in this space. If we knew this set of factors, equivalent to a unit vector in this space, we could filter out the contents of that signal from data that could have been generated by more than one source.

In other words, if we assume that the signal from every source is picked up by a measurement device, after being scaled by a source- and measurement device-dependent factor, and that signals from different sources merely add, then we can use ICA to tell apart the signals from different sources.

We haven't yet specified a criterion for the transformed representation. To do that, we'll make another assumption that seems innocuous: that observing the signal from one source should tell us absolutely nothing about the signal from another source. Surprisingly, this requirement can be turned into the mathematical criterion we need. 

As a side note: in many explanations of ICA, this criterion is stated as "assuming the sources are statistically independent" or even "maximizing the non-Gaussianity of the sources". To me, these descriptions are confusing. I think they come from looking at the math/code of ICA for too long, then trying to map back what you're doing into English. But you're really mapping into math-speak, which isn't understandable to most people, at least not before they've grokked the mathematics. But for them make the effort to grok the mathematics, they need to fully understand the motivation--it's a vicious cycle.

Let's make the criterion we've specified more concrete: in the scenario where we're trying to distinguish the voices of different people in the same room by using the measurments of various microphones scattered throughout the room, our criterion is that what Alice is saying has absolutely nothing to do with what Bob is saying. In other words, by paying attention to what Alice says, we are no better off in predicting what Bob is saying. The signal from one source doesn't help us predict the signal from another source--this is what is meant by "the sources are statistically independent".

How can we make this into something mathematical we can code up? By using the language of probability. We're observing a signal $x_i$ from multiple sources $s_i$. At any given instant, the signals we pick up from every source can be combined into a vector $\mathbf{x}$, where each element is one of the signals $x_i$. Think of the vector $\mathbf{x}$ as being drawn from a multivariate joint probability distribution $P(\mathbf{x})$.

Now, this is the key point: if the sources were generating truly independent signals, then the multivariate probability distribution $P(\mathbf{x})$, from which you draw a vector of all measurements, would _factorize_ into the product of single-variable probability distributions for each source. Mathematically,

\begin{equation}
P(\mathbf{x}) = P_1(x_1) \cdot P_2(x_2) \cdot \cdots \cdot P_m(x_m) = \prod_{i=1}^{m} P_i(x_i)
\end{equation}

where each $P_i$ is the probability distribution you draw from when observing the signal from source $i$, and where $m$ is the number of sources.

This suggests that the mathematical criterion we look for in the transformed data is complete factorization of the joint probability distribution over the transformed variables $x_i$. However, although the criterion is now stated in terms of math, it's not clear how to evaluate whether it's satisfied, much less look for a transformation that satisfies the criterion. We'll see how to accomplish those tasks when we get to implementing ICA.
