Independent Component Analysis (ICA) is an unsupervised learning method, and if you haven't heard of it, you've probably heard of its more famous cousin Principle Component Analysis (PCA). PCA has a high ratio of usefulness to mathematical difficulty, and it's often introduced as a paradigmatic example of unsupervised learning. ICA is not as straightforward, but that also means it seems even more like magic. Consider this picture from Hastie, Tibshirani, Friedman [1]:

![ICA vs PCA](/images/ICA_PCA.png)

Clearly PCA is hopeless at separating the two signals, whereas ICA recovers them perfectly. Given that PCA is usually presented alongside examples that clearly illustrate its utility, and make PCA itself seem magical, this example only made ICA seem even more mysterious and magical. Furthermore, I was wondering if it was related to PCA--their names couldn't just be a coincidence, right? So I decided to dive in, and write up my thoughts here. 


## Motivation

Q: Why would one need to know how to do ICA? 

A: Well, it's a basic form of unsupervised learning. 

Q: Why would one need to know how to do unsupervised learning?

I'm going to motivate ICA by first providing my answer to the second question. 

### Unsupervised learning

What is unsupervised learning? One way to think about it is as a transformation process. You are given some data, and you want to transform it to another representation, where you can "interpret" it better. What does "interpret" mean? It means that you assign meaning to that representation of the data. In other words, you create a higher-level abstraction, and use your label for that abstraction to stand for some of the data. You're using something simpler, like a word, to represent something unwieldy, like a relationship between lots and lots of numbers. 

You do this so that in the future, if you're reasoning about something else and this abstraction happens to play a role, you can plug in your labeled data, which you used unsupervised learning to isolate.

Here's a concrete example of this: You're in kindergarten. The teacher is saying something about an "alphabet", and shows you an image with weird drawings on it. This is your data. You learn that there are 26 drawings, you learn to tell each of those drawings apart--this is you doing unsupervised learning on your data. Then, your teacher creates the abstractions of the letters A-Z, and helped you assign a label--the name of a letter--to each of the 26 different shapes. You're using something simple, like the sound "ay", to stand for the up-pointing shape with a line through the middle.

In first grade, let's say you need to write down your name. If you know that your name starts with the letter A, you're all set. The unsupervised learning you did in kindergarten means that you know what visual data corresponds to the abstraction A--an up-pointing shape with a line through the middle. You draw this on your paper, and then loop through the rest of the letters in your name.

### ICA

That was a really high-level discussion, so let's try to get more concrete. Above, I said unsupervised learning is a transformation process. Given some data, you want to _transform_ it to another _representation_. There are two avenues to making this statement more concrete: we can answer "transform it how?" and "what criteria do you want this other representation to have?"

#### Transform it how?

The simplest thing to consider at first might be linear transforms. This means that given some data, we're only going to transform it by multiplying the numbers in that data by some constants, and/or adding the results of those multiplications together. Another way to understand this is by representing each data point as a vector, where the elements of the vector are the values of each attribute of the data point (I'm assuming multi-dimensional data for this post). By restricting yourself to linear transforms, you're saying that you're only going to multiply this vector by a matrix, and use the same matrix for all your data points. (The matrix itself can change depending on the entire data set, though).

#### What criteria for the representation?

Next, we have to specify the characteristics we want the transformed representation of the data to have. This is the interesting part. Behind many transformations are assumptions: you act as though a certain scenario is true, which allows you to actually do something to your data, and then when you're finished transforming the data, you check to see if the output is actually consistent with the assumptions you made in the beginning. This is also the case for ICA.

We're going to assume that the data is generated by a number of 'sources'. Each source outputs a signal that various measurement devices of ours pick up. You can think of the reading of each measurement device as one element of our data. For example, each 'source' could be the voice of certain people in a room, and our measurement devices could be a series of microphones [2]. Each microphone is placed in a different location in the room. Since every person is a different distance from all of the microphones, each source is picked up in a unique way by each of the microphones. Mathematically, a signal from a given source is attenuated by a constant factor when it is picked up by a given microphone. For a given source, the set of these factors tell you how our measurement devices pick up its signal. If you consider the vector space where our data lives, this set of factors picks out a unique direction in this space. If we knew this set of factors, equivalent to a unit vector in this space, we could filter out the contents of that signal from data that could have been generated by more than one source.

In other words, if we assume that the signal from every source is picked up by a measurement device 
    
1. after being scaled by a source- and measurement device- dependent factor, and 
2. that signals from different sources merely add, 

then we can use ICA to tell apart the signals from different sources.

---

We haven't yet specified a criterion for the transformed representation. To do that, we'll make another innocuous-seeming assumption: that observing the signal from one source should tell us absolutely nothing about the signal from another source. Surprisingly, this requirement can be turned into the mathematical criterion we need. 

As a side note: in many explanations of ICA, this criterion is stated as "assuming the sources are statistically independent" or even "maximizing the non-Gaussianity of the sources". To me, these descriptions are confusing. I think they come from looking at the math/code of ICA for too long, then trying to map back what you're doing into English. But you're really mapping into math-speak, which isn't understandable to most people, at least not before they've grokked the mathematics. But for them make the effort to grok the mathematics, they need to fully understand the motivation--it's a vicious cycle.

Let's make this criterion more concrete: in the scenario where we're trying to distinguish the voices of different people in the same room by using the measurements of various microphones scattered throughout the room, our criterion is that what Alice is saying has absolutely nothing to do with what Bob is saying. In other words, by paying attention to what Alice says, we are no better off in predicting what Bob is saying. The signal from one source doesn't help us predict the signal from another source--this is what is meant by "the sources are statistically independent".

How can we make this into something mathematical we can code up? By using the language of probability. We're observing a signa from multiple sources $s_i^R$. (Since these are random variables, I'm putting an $R$ next to them.) At any given instant, the signals from every source can be combined into a vector $\mathbf{s}^R$, where each element is one of the signals $s_i^R$. Think of the vector $\mathbf{s}^R$ as being drawn from a multivariate probability distribution $P(\mathbf{s}^R)$.

Now, the key point: if the sources were generating truly independent signals, then the multivariate probability distribution $P(\mathbf{s}^R)$, from which you draw a vector consisting of all the measurements, would _factorize_ into the product of single-variable probability distributions for each source. Mathematically,

\begin{equation}
P(\mathbf{s}^R) = P_1(s_1^R) \cdot P_2(s_2^R) \cdot \cdots \cdot P_m(s_m^R)
\end{equation}

where each $P_i$ is the probability distribution you draw from when measuring the signal from source $i$, and where $m$ is the number of sources.

This suggests that the mathematical criterion we look for in the transformed data is complete factorization of the joint probability distribution over the variables $s_i^R$. However, although the criterion is now stated in terms of math, it's not clear how to evaluate whether it's satisfied, much less look for a transformation that satisfies the criterion. We'll see how to accomplish those tasks when we get to implementing ICA.


## References

[1] Hastie, Tibshirani, and Friedman, _The Elements of Statistical Learning_

[2] Shlens, A Tutorial on Independent Component Analysis. arXiv:1404.2986v1. The voices/microphones scenario is a common one used to motivate ICA (it's an example of a more general problem called blind-source separation)
